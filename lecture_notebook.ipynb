{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d5701b",
   "metadata": {},
   "source": [
    "# Quantitative Equity Portfolio Architecture: Lecture Series\n",
    "\n",
    "Welcome to the interactive module on **Quantitative Portfolio Optimization**, built around Python, PyPortfolioOpt, SciPy, and Pandas.\n",
    "\n",
    "In this notebook, we decompose the complete structure of a modern quantitative application into educational segments, explaining the mathematical foundations behind each step.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Learning Objectives\n",
    "1. **Data Ingestion & Integrity:** Utilizing `pandas.DatetimeIndex` frameworks safely.\n",
    "2. **Matrix Condition & Optimization:** How Ledoit-Wolf shrinkage stabilizes the Efficient Frontier matrix.\n",
    "3. **Parametric Risk Measurement:** Implementing Value at Risk (VaR) and Conditional VaR (CVaR).\n",
    "4. **Stochastic Processes:** Simulating Geometric Brownian Motion (GBM) dynamically via high-performance hardware-accelerated NumPy matrices avoiding traditional logic loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from pypfopt import risk_models, expected_returns, EfficientFrontier\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c13a1",
   "metadata": {},
   "source": [
    "## 1. Data Integrity and Smart Caching\n",
    "\n",
    "In financial engineering, preventing data leakage and minimizing I/O overhead on external APIs (like Yahoo Finance) is crucial.\n",
    "The `fetch_data` function guarantees that multiple ticker symbols map properly to uniform `float64` boundaries. Missing values on holidays or weekends are forward/backward filled securely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2484ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FILE = \"price_cache.parquet\"\n",
    "\n",
    "def fetch_data(tickers: list[str], start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches adjusted close prices using a local pyarrow/parquet cache.\n",
    "    \n",
    "    Mathematical/Data Schema:\n",
    "    - Returns: pandas.DataFrame\n",
    "    - Index: pandas.DatetimeIndex\n",
    "    - Columns: Ticker symbols (str)\n",
    "    - Values: Adjusted Close prices (float64) representing the asset value $S_t$.\n",
    "    \"\"\"\n",
    "    tickers = [t.upper() for t in tickers]\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    \n",
    "    if end_dt.date() == datetime.today().date():\n",
    "         end_dt = end_dt + timedelta(days=1)\n",
    "\n",
    "    cached_df = pd.DataFrame()\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        cached_df = pd.read_parquet(CACHE_FILE)\n",
    "\n",
    "    tickers_to_fetch = []\n",
    "    fetch_start, fetch_end = start_dt, end_dt\n",
    "\n",
    "    if not cached_df.empty:\n",
    "        cached_tickers = cached_df.columns.tolist()\n",
    "        tickers_to_fetch = [t for t in tickers if t not in cached_tickers]\n",
    "        \n",
    "        cached_start, cached_end = cached_df.index.min(), cached_df.index.max()\n",
    "        if start_dt < cached_start or end_dt > cached_end + timedelta(days=1):\n",
    "             tickers_to_fetch = tickers\n",
    "             fetch_start, fetch_end = start_dt, end_dt\n",
    "    else:\n",
    "        tickers_to_fetch = tickers\n",
    "\n",
    "    if tickers_to_fetch:\n",
    "        print(f\"Fetching {tickers_to_fetch} from yfinance...\")\n",
    "        df = yf.download(tickers_to_fetch, start=fetch_start, end=fetch_end, auto_adjust=False)\n",
    "        \n",
    "        new_data = pd.DataFrame()\n",
    "        if len(tickers_to_fetch) == 1 and 'Adj Close' in df.columns:\n",
    "            new_data = df[['Adj Close']].copy()\n",
    "            new_data.columns = [tickers_to_fetch[0]]\n",
    "        elif df.columns.nlevels > 1 and 'Adj Close' in df.columns.levels[0]:\n",
    "            new_data = df['Adj Close'].copy()\n",
    "                \n",
    "        if not new_data.empty:\n",
    "            if new_data.index.tz is not None:\n",
    "                new_data.index = new_data.index.tz_localize(None)\n",
    "                \n",
    "            if not cached_df.empty and tickers_to_fetch != tickers:\n",
    "                cached_df = pd.concat([cached_df, new_data], axis=1)\n",
    "            elif not cached_df.empty:\n",
    "                # Transpose, groupby to deduplicate overlap, transpose back (support pandas 2.1+)\n",
    "                cached_df = pd.concat([cached_df, new_data], axis=1).T.groupby(level=0).first().T\n",
    "            else:\n",
    "                cached_df = new_data\n",
    "            \n",
    "            cached_df.sort_index(inplace=True)\n",
    "            cached_df.to_parquet(CACHE_FILE)\n",
    "\n",
    "    if not cached_df.empty:\n",
    "        mask = (cached_df.index >= pd.to_datetime(start_date)) & (cached_df.index <= pd.to_datetime(end_date))\n",
    "        result_df = cached_df.loc[mask]\n",
    "        result_df = result_df[[t for t in tickers if t in result_df.columns]].ffill().bfill().astype('float64')\n",
    "        return result_df\n",
    "    \n",
    "    return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2934be1",
   "metadata": {},
   "source": [
    "## 2. Mathematical Engines: Portfolio Optimization\n",
    "\n",
    "Let $\\mu$ be the vector of exponentially-weighted historical returns, and $\\Sigma$ the sample covariance matrix.\n",
    "According to Markowitz' Modern Portfolio Theory (MPT), an optimizer maps weights array $w$ to define the Efficient Frontier.\n",
    "\n",
    "**Maximum Sharpe Ratio:**\n",
    "$$ \\max_w \\frac{w^T \\mu - R_f}{\\sqrt{w^T \\Sigma w}} $$\n",
    "\n",
    "**Numerical Stability (Ledoit-Wolf Shrinkage):**\n",
    "Financial datasets often yield poorly scaled covariance matrices $cond(\\Sigma) \\gg 1$.\n",
    "To prevent optimization algorithms from failing, we \"shrink\" the sample covariance against a structured target $F$:\n",
    "$$ \\Sigma_{shrink} = (1-\\delta)\\Sigma_{sample} + \\delta F $$\n",
    "(Ledoit & Wolf, 2004).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10bcf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns_and_cov(df: pd.DataFrame) -> tuple[pd.Series, pd.DataFrame]:\n",
    "    \"\"\"Calculates EMA returns and applies Ledoit-Wolf shrinkage to standard covariance.\"\"\"\n",
    "    mu = expected_returns.ema_historical_return(df)\n",
    "    S = risk_models.CovarianceShrinkage(df).ledoit_wolf()\n",
    "    return mu, S\n",
    "\n",
    "def optimize_portfolio(df: pd.DataFrame, strategy: str = \"max_sharpe\", target: float = None) -> pd.Series:\n",
    "    \"\"\"Invokes convex optimization solver (via cvxpy backend).\"\"\"\n",
    "    mu, S = get_returns_and_cov(df)\n",
    "    ef = EfficientFrontier(mu, S)\n",
    "    \n",
    "    if strategy == \"max_sharpe\":\n",
    "        weights = ef.max_sharpe()\n",
    "    elif strategy == \"min_volatility\":\n",
    "        weights = ef.min_volatility()\n",
    "    elif strategy == \"efficient_return\":\n",
    "        weights = ef.efficient_return(target_return=target or mu.mean())\n",
    "    elif strategy == \"max_sortino\":\n",
    "        semi_cov = risk_models.semicovariance(df)\n",
    "        ef = EfficientFrontier(mu, semi_cov)\n",
    "        weights = ef.max_sharpe()\n",
    "        \n",
    "    cleaned_weights = ef.clean_weights()\n",
    "    return pd.Series(cleaned_weights).astype('float64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccc40b",
   "metadata": {},
   "source": [
    "## 3. Parametric Risk Estimation\n",
    "\n",
    "**Value at Risk (VaR)** estimates the maximum potential loss at a given confidence interval $\\alpha$ assuming normality.\n",
    "$$ VaR_{\\alpha} = - (w^T\\mu - z_{\\alpha} \\sqrt{w^T\\Sigma w}) $$\n",
    "\n",
    "**Conditional Value at Risk (CVaR / Expected Shortfall)** is the expected loss *given* that the VaR threshold is breached.\n",
    "$$ CVaR_{\\alpha} = - \\left( w^T\\mu - \\sqrt{w^T\\Sigma w} \\frac{\\phi(z_{\\alpha})}{\\alpha} \\right) $$\n",
    "where $\\phi$ is the normal PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cafbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_var_cvar(weights: pd.Series, df: pd.DataFrame, alpha: float = 0.05) -> tuple[float, float]:\n",
    "    returns = df.pct_change().dropna()\n",
    "    mu, S = get_returns_and_cov(df)\n",
    "    \n",
    "    # Ensuring weight padding to prevent alignment faults if optimizer pruned a ticker\n",
    "    w = weights.reindex(returns.columns, fill_value=0.0).values\n",
    "    \n",
    "    mu_p = np.dot(w, mu)\n",
    "    sigma_p = np.sqrt(np.dot(w.T, np.dot(S, w)))\n",
    "    \n",
    "    z_alpha = norm.ppf(1 - alpha)\n",
    "    \n",
    "    var_loss = -(mu_p - z_alpha * sigma_p)\n",
    "    cvar_loss = -(mu_p - sigma_p * (norm.pdf(z_alpha) / alpha))\n",
    "    \n",
    "    return float(var_loss), float(cvar_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205786f",
   "metadata": {},
   "source": [
    "## 4. Vectorized Geometric Brownian Motion\n",
    "\n",
    "To assess deep stochastic pathways, we simulate the Geometric Brownian Motion (GBM).\n",
    "Asset price $S_t$ evolves over bounded steps:\n",
    "$$ S_{t+\\Delta t} = S_t \\exp\\left( \\left(\\mu - \\frac{\\sigma^2}{2}\\right)\\Delta t + \\sigma \\sqrt{\\Delta t} Z \\right) $$\n",
    "\n",
    "For $\\Delta t = 1$ day, we draw $Z \\sim \\mathcal{N}(0,1)$ concurrently across $N$ simulations using hardware-optimized memory allocation avoiding native Python `for` loop bottlenecks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61959d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_monte_carlo_gbm(weights: pd.Series, df: pd.DataFrame, days_ahead: int = 252, n_simulations: int = 10000) -> pd.DataFrame:\n",
    "    mu, S = get_returns_and_cov(df)\n",
    "    w = weights.reindex(df.columns, fill_value=0.0).values\n",
    "    \n",
    "    # Portfolio expected daily parameters scaling annual frequency\n",
    "    mu_p_daily = np.dot(w, mu) / 252.0\n",
    "    sigma_p_daily = np.sqrt(np.dot(w.T, np.dot(S, w))) / np.sqrt(252.0)\n",
    "    \n",
    "    dt = 1 \n",
    "    Z = np.random.standard_normal((days_ahead, n_simulations))\n",
    "    \n",
    "    drift = (mu_p_daily - 0.5 * sigma_p_daily**2) * dt\n",
    "    diffusion = sigma_p_daily * np.sqrt(dt) * Z\n",
    "    \n",
    "    daily_returns = np.exp(drift + diffusion)\n",
    "    initial_value = np.ones((1, n_simulations))\n",
    "    \n",
    "    paths = np.vstack([initial_value, daily_returns])\n",
    "    cumulative_paths = np.cumprod(paths, axis=0)[1:, :]\n",
    "    \n",
    "    return pd.DataFrame(cumulative_paths, index=np.arange(1, days_ahead + 1), dtype='float64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c23f83",
   "metadata": {},
   "source": [
    "## 5. Execution Pipeline Example\n",
    "\n",
    "Finally, we bind the functions together. Here we fetch data for a basket of equities, optimize weights for the **Maximum Sharpe Ratio**, and plot the Monte Carlo density curve!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e692c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pipeline Start\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'SPY', 'TLT']\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "df = fetch_data(tickers, start_date, end_date)\n",
    "print(f\"Loaded {df.shape[0]} business days.\")\n",
    "\n",
    "# 2. Optimize Portfolio\n",
    "weights = optimize_portfolio(df, \"max_sharpe\")\n",
    "print(\"\\nOptimal Weights (Max Sharpe):\")\n",
    "print(weights[weights > 0.01].round(4) * 100)\n",
    "\n",
    "var, cvar = calculate_var_cvar(weights, df)\n",
    "print(f\"\\nParametric VaR (95%): {var*100:.2f}% | CVaR: {cvar*100:.2f}%\")\n",
    "\n",
    "# 3. Running GBM Simulation natively in Jupyter\n",
    "simulations = run_monte_carlo_gbm(weights, df, days_ahead=252, n_simulations=5000)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot first 100 paths\n",
    "for i in range(100):\n",
    "    fig.add_trace(go.Scatter(x=simulations.index, y=simulations.iloc[:, i], mode='lines', \n",
    "                             line=dict(color='gray', width=1), opacity=0.1, showlegend=False))\n",
    "\n",
    "# Plot Percentiles Overlay\n",
    "percentiles = {1: 'red', 50: 'green', 99: 'red'}\n",
    "for p, color in percentiles.items():\n",
    "    p_vals = simulations.apply(lambda x: np.percentile(x, p), axis=1)\n",
    "    fig.add_trace(go.Scatter(x=simulations.index, y=p_vals, mode='lines', \n",
    "                             name=f'{p}th Pct', line=dict(color=color, width=2, dash='solid' if p == 50 else 'dash')))\n",
    "\n",
    "fig.update_layout(title=\"Monte Carlo GBM Density Bounds\", yaxis_title=\"Normalized Value\", template=\"plotly_white\")\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
